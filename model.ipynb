{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f64cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d6e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37daa322",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import Ridge, LinearRegression, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e336c83",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f087b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N):\n",
    "    \"\"\"\n",
    "    Generates a data matrix X and target vector y with specified properties.\n",
    "\n",
    "    Args:\n",
    "        N (int): Number of data points.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the data matrix X (torch.Tensor) with N rows and 101 columns,\n",
    "               and the target vector y (torch.Tensor) with N rows and 1 column.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(N, int) or N <= 0:\n",
    "        raise ValueError(\"N must be a positive integer.\")\n",
    "\n",
    "    X = torch.zeros(N, 101, dtype=torch.float32)\n",
    "\n",
    "    for i in range(N):\n",
    "        z_0 = torch.randn(1).detach() + 1\n",
    "        z_rest = torch.randn(100)\n",
    "        X[i] = torch.cat((z_0, z_rest))\n",
    "\n",
    "    y_mu = torch.zeros(N)\n",
    "    for i in range(N):\n",
    "        y_mu[i] = (X[i, 0] + X[i, -1]) / 2\n",
    "\n",
    "    return X, y_mu\n",
    "\n",
    "\n",
    "generate_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b3f177",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b17985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_prediction(X, weights):\n",
    "    y_pred = torch.matmul(X, weights)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def squared_error_loss(y_actual, y_pred):\n",
    "    squared_errors = (y_actual - y_pred) ** 2\n",
    "\n",
    "    # Compute average squared error loss\n",
    "    loss = torch.mean(squared_errors)\n",
    "\n",
    "    return loss\n",
    "\n",
    "N, D = 100, 10\n",
    "X = torch.randn(N, D)\n",
    "\n",
    "weights = torch.randn(D, 1)\n",
    "y_actual = torch.randn(N, 1)\n",
    "\n",
    "# Compute predictions\n",
    "y_pred = linear_model_prediction(X, weights)\n",
    "\n",
    "# Compute loss\n",
    "loss = squared_error_loss(y_actual, y_pred)\n",
    "\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983adf1",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform linear regression using gradient descent\n",
    "def linear_regression(X, y, alpha, num_epochs):\n",
    "    num_samples, num_features = X.shape\n",
    "    theta = np.zeros((num_features, 1))\n",
    "    bias = 0\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        y_pred = np.dot(X, theta) + bias\n",
    "\n",
    "        # Compute loss\n",
    "        train_loss = np.mean((y_pred - y) ** 2)\n",
    "        test_loss = np.mean((np.dot(X_test, theta) + bias - y_test) ** 2)\n",
    "        train_loss_history.append(train_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "\n",
    "        # Gradient descent\n",
    "        theta_gradient = (2 / num_samples) * np.dot(X.T, (y_pred - y))\n",
    "        bias_gradient = (2 / num_samples) * np.sum(y_pred - y)\n",
    "\n",
    "        theta -= alpha * theta_gradient\n",
    "        bias -= alpha * bias_gradient\n",
    "\n",
    "    return theta, bias, train_loss_history, test_loss_history\n",
    "\n",
    "# Generic data\n",
    "np.random.seed(0)\n",
    "N_train = 1000\n",
    "N_test = 200\n",
    "X_train = np.random.randn(N_train, 1)\n",
    "y_train = 3 * X_train + np.random.randn(N_train, 1)\n",
    "X_test = np.random.randn(N_test, 1)\n",
    "y_test = 3 * X_test + np.random.randn(N_test, 1)\n",
    "\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "num_epochs = 1000\n",
    "\n",
    "# Plotting training and testing loss for different step sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "for alpha in alphas:\n",
    "    _, _, train_loss_history, test_loss_history = linear_regression(X_train, y_train, alpha, num_epochs)\n",
    "    plt.plot(range(num_epochs), train_loss_history, label=f'Train (alpha={alpha})')\n",
    "    plt.plot(range(num_epochs), test_loss_history, '--', label=f'Test (alpha={alpha})')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss over Time for Different Step Sizes')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "''' To end the training loop, you can use a termination condition such as reaching a maximum number of iterations,\n",
    " achieving a satisfactory level of loss convergence, or observing negligible improvement in loss over several iterations.  '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ae358",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, alpha, num_iterations):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        y_pred = np.dot(X, theta)\n",
    "        gradient = np.dot(X.T, (y_pred - y)) / m\n",
    "        theta -= alpha * gradient\n",
    "\n",
    "        train_loss = np.mean((y_pred - y) ** 2)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        y_test_pred = np.dot(X_test, theta)\n",
    "        test_loss = np.mean((y_test_pred - y_test) ** 2)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    return theta, train_losses, test_losses\n",
    "\n",
    "def has_converged(losses, threshold=1e-5):\n",
    "    return np.abs(losses[-1] - losses[-2]) < threshold if len(losses) > 1 else False\n",
    "\n",
    "# Generating synthetic data\n",
    "np.random.seed(0)\n",
    "X_train = np.random.rand(1000, 1)\n",
    "y_train = 2 * X_train.squeeze() + np.random.randn(1000)\n",
    "\n",
    "X_test = np.random.rand(200, 1)\n",
    "y_test = 2 * X_test.squeeze() + np.random.randn(200)\n",
    "\n",
    "# Concatenating bias term\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "\n",
    "# Step 1: Experiment with Step Sizes\n",
    "step_sizes = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "converged_models = []\n",
    "\n",
    "for alpha in step_sizes:\n",
    "    theta, train_losses, test_losses = linear_regression(X_train, y_train, alpha, num_iterations=1000)\n",
    "    if has_converged(train_losses):\n",
    "        converged_models.append((alpha, theta, train_losses, test_losses))\n",
    "\n",
    "# Step 2: Plot Loss on Training and Testing Data\n",
    "for alpha, theta, train_losses, test_losses in converged_models:\n",
    "    plt.plot(range(len(train_losses)), train_losses, label=f'Train Loss (alpha={alpha})')\n",
    "    plt.plot(range(len(test_losses)), test_losses, label=f'Test Loss (alpha={alpha})')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Evaluate Generalization to Testing Data\n",
    "best_alpha, best_theta, _, test_losses = converged_models[0]\n",
    "train_loss_final = train_losses[-1]\n",
    "test_loss_final = test_losses[-1]\n",
    "print(f\"Best Alpha: {best_alpha}\")\n",
    "print(f\"Final Training Loss: {train_loss_final}\")\n",
    "print(f\"Final Testing Loss: {test_loss_final}\")\n",
    "\n",
    "# Step 4: Compare with Actual Model (In this case, the actual model is known: y = 2x)\n",
    "y_test_actual = 2 * X_test[:, 1]\n",
    "y_test_pred = np.dot(X_test, best_theta)\n",
    "mse = np.mean((y_test_pred - y_test_actual) ** 2)\n",
    "print(f\"MSE with Actual Model: {mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acb4b7",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9942203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X_train = np.random.rand(1000, 1)\n",
    "y_train = 2 * X_train.squeeze() + np.random.randn(1000)\n",
    "\n",
    "X_test = np.random.rand(200, 1)\n",
    "y_test = 2 * X_test.squeeze() + np.random.randn(200)\n",
    "\n",
    "# Concatenate bias term\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "\n",
    "# Train two different linear regression models with different initializations\n",
    "theta_1, _, _ = linear_regression(X_train, y_train, alpha=0.01, num_iterations=1000)\n",
    "theta_2, _, _ = linear_regression(X_train, y_train, alpha=0.01, num_iterations=1000)\n",
    "\n",
    "# Evaluate the models on the testing data\n",
    "y_test_pred_1 = np.dot(X_test, theta_1)\n",
    "y_test_pred_2 = np.dot(X_test, theta_2)\n",
    "\n",
    "# Compare the predictions\n",
    "print(\"Model 1 Predictions:\", y_test_pred_1[:5])\n",
    "print(\"Model 2 Predictions:\", y_test_pred_2[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640337e",
   "metadata": {},
   "source": [
    "### The fact that both models yield similar predictions indicates that there is no single \"best\" model. Even though the models have different parameter settings, they are equally effective in capturing the underlying relationship between the features and the target variable. This highlights the concept that there exists a set of models that fit the data equally well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d09ff",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31865a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(N):\n",
    "    X = np.random.rand(N, 1) * 10\n",
    "    y = np.sin(X) + np.random.randn(N, 1) * 0.1  # Adding noise\n",
    "    return X, y\n",
    "\n",
    "# Define the neural network model\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "def train_model(train_X, train_y, test_X, test_y, model, epochs=100, batch_size=32):\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_X, test_y), verbose=0)\n",
    "    return history.history['loss'], history.history['val_loss']\n",
    "\n",
    "# Define the sizes of training data\n",
    "training_sizes = [1000, 500, 250, 100, 50]\n",
    "test_size = 200\n",
    "\n",
    "# Generate fixed test set\n",
    "test_X, test_y = generate_data(test_size)\n",
    "\n",
    "# Train models for different training sizes\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for size in training_sizes:\n",
    "    train_X, train_y = generate_data(size)\n",
    "    model = build_model()\n",
    "    train_loss, test_loss = train_model(train_X, train_y, test_X, test_y, model)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "# Plot the training and testing losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, size in enumerate(training_sizes):\n",
    "    plt.plot(train_losses[i], label=f'Train (N={size})')\n",
    "    plt.plot(test_losses[i], label=f'Test (N={size})', linestyle='--')\n",
    "\n",
    "plt.title('Training and Testing Losses for Different Training Set Sizes')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa3eddd",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d0d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 300\n",
    "N_test = 900\n",
    "X_train = np.random.randn(N_train, 10)\n",
    "y_train = np.random.randn(N_train)\n",
    "X_test = np.random.randn(N_test, 10)\n",
    "y_test = np.random.randn(N_test)\n",
    "\n",
    "lambda_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "testing_loss = []\n",
    "\n",
    "for lambda_val in lambda_values:\n",
    "\n",
    "    ridge_model = Ridge(alpha=lambda_val)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    y_pred = ridge_model.predict(X_test)\n",
    "    loss = mean_squared_error(y_test, y_pred)\n",
    "    testing_loss.append(loss)\n",
    "\n",
    "# Plot testing loss as a function of lambda\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(lambda_values, testing_loss, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda (log scale)')\n",
    "plt.ylabel('Testing Loss')\n",
    "plt.title('Testing Loss vs. Lambda')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeef4dd",
   "metadata": {},
   "source": [
    "# Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training and testing data\n",
    "N_train = 300\n",
    "N_test = 200\n",
    "num_features = 10\n",
    "X_train = np.random.randn(N_train, num_features)  # Example: 10 features\n",
    "y_train = np.random.randn(N_train)\n",
    "X_test = np.random.randn(N_test, num_features)\n",
    "y_test = np.random.randn(N_test)\n",
    "\n",
    "# Define a range of beta values to test\n",
    "beta_values = np.linspace(0.01, 1, 100)\n",
    "\n",
    "# Initialize lists to store number of non-zero weights and zeroing points for each feature\n",
    "non_zero_weights = []\n",
    "zeroing_points = [[] for _ in range(num_features)]\n",
    "\n",
    "# Fit lasso regression models for each beta value and compute number of non-zero weights\n",
    "for beta_val in beta_values:\n",
    "    lasso_model = Lasso(alpha=beta_val)\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "\n",
    "    coefficients = lasso_model.coef_\n",
    "    num_non_zero = np.sum(coefficients != 0)\n",
    "    non_zero_weights.append(num_non_zero)\n",
    "\n",
    "    for i in range(num_features):\n",
    "        if coefficients[i] == 0:\n",
    "            zeroing_points[i].append(beta_val)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(beta_values, non_zero_weights)\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Number of Non-Zero Weights')\n",
    "plt.title('Number of Non-Zero Weights vs. Beta')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(num_features):\n",
    "    plt.plot(zeroing_points[i], label=f'Feature {i+1}')\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Zeroing Points')\n",
    "plt.title('Zeroing Points for Each Feature')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4f507",
   "metadata": {},
   "source": [
    "# Problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training and testing data\n",
    "N_train = 300\n",
    "N_test = 200\n",
    "num_features = 101\n",
    "X_train = np.random.randn(N_train, num_features)\n",
    "y_train = np.random.randn(N_train)\n",
    "X_test = np.random.randn(N_test, num_features)\n",
    "y_test = np.random.randn(N_test)\n",
    "\n",
    "k_values = range(1, 51)\n",
    "\n",
    "num_trials = 10\n",
    "\n",
    "average_testing_errors = []\n",
    "\n",
    "\n",
    "for k in k_values:\n",
    "\n",
    "    testing_errors = []\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        # Generate random matrix A with k rows and 101 columns\n",
    "        A = np.random.randn(k, num_features)\n",
    "\n",
    "        X_train_transformed = np.dot(X_train, A.T)\n",
    "        X_test_transformed = np.dot(X_test, A.T)\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "        testing_error = mean_squared_error(y_test, y_pred)\n",
    "        testing_errors.append(testing_error)\n",
    "\n",
    "    average_testing_error = np.mean(testing_errors)\n",
    "    average_testing_errors.append(average_testing_error)\n",
    "\n",
    "# Plot average testing error as a function of k\n",
    "plt.plot(k_values, average_testing_errors)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Average Testing Error')\n",
    "plt.title('Average Testing Error vs. k')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20abde0",
   "metadata": {},
   "source": [
    "# Problem 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features_max = 200\n",
    "n_repeats = 10\n",
    "alphas = np.logspace(-6, 6, 100)  # Range of alpha values for Ridge regression\n",
    "\n",
    "testing_errors = []\n",
    "\n",
    "for k in range(50, n_features_max + 1):\n",
    "    k_errors = []\n",
    "    for _ in range(n_repeats):\n",
    "        # Generate random data\n",
    "        X_train = np.random.randn(n_samples, k)\n",
    "        y_train = np.random.randn(n_samples)\n",
    "        X_test = np.random.randn(n_samples, k)\n",
    "        y_test = np.random.randn(n_samples)\n",
    "\n",
    "        # Perform Ridge regression with cross-validation to find optimal alpha\n",
    "        ridge = RidgeCV(alphas=alphas, cv=5)\n",
    "        ridge.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = ridge.predict(X_test)\n",
    "\n",
    "        # Calculate testing error (MSE)\n",
    "        error = mean_squared_error(y_test, y_pred)\n",
    "        k_errors.append(error)\n",
    "\n",
    "    # Average testing errors for this value of k\n",
    "    testing_errors.append(np.mean(k_errors))\n",
    "\n",
    "# Plot the testing errors\n",
    "plt.plot(range(50, n_features_max + 1), testing_errors)\n",
    "plt.xlabel('Number of features (k)')\n",
    "plt.ylabel('Testing Error (MSE)')\n",
    "plt.title('Testing Error vs. Number of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796727c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
